---
aliases: []
author: Maneesh Sutar
created: 2025-01-18
modified: 2025-01-19
tags: []
title: 2025_01_18.md
---

## TIL

**NVIDIA GPUS:**

1. Grace Hopper - GH200 - https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper
1. Grace SuperChip https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-cpu-superchip#page=1

**CUDA Unified Memory Programming**

1. Useful in newer NVIDIA GPUs (Hopper+) and CPUs (Grace) which support unified memory access across GPU/CPU on a single PCB + GPU/CPU connected across PCBs using NVIDIA NVLINK
1. Because of unified memory, GPUs can use CPU ram as extra memory if their in built memory gets full
1. https://docs.nvidia.com/cuda/cuda-c-programming-guide/#unified-memory-programming

**CUDA Aware MPI**

1. [Original blog post](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/)
1. Basic idea: Support direct peer-to-peer GPU data transfers in multi-host systems
1. NVIDIA GPUs can skip the host memory and either:
   1. Directly send data to another GPU on the same host via IO chipset
   1. Directly write data to network card buffer (Ethernet of Infiniband), and also read from network card
1. This is kind of similar to Nvidia's NVLINK bridges for Grace Hopper, while being non-proprietary and it also works on older NVIDIA GPUs
1. OpenMPI support [CUDA-Aware MPI](https://docs.open-mpi.org/en/v5.0.x/tuning-apps/networking/cuda.html#how-do-i-build-open-mpi-with-cuda-aware-support).
1. The kernel code must be written in CUDA or NCCL (see [GPU programming](../GPU/gpu_programming.md)). Other paradigms like OpenCL and HIP do not support CUDA aware MPI.
